{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1_00_notMNIST_small.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"B_iAjwPm_mYg"},"source":["# Letter recognition (small size)\n","\n","> Indeed, I once even proposed that the toughest challenge facing AI workers is to answer the question: “What are the letters ‘A’ and ‘I’? - [Douglas R. Hofstadter](https://web.stanford.edu/group/SHR/4-2/text/hofstadter.html) (1995)\n","\n","\n","## notMNIST\n","\n","\n","Data source: [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) (you need to download `notMNIST_small.mat` file):\n","\n","![](http://yaroslavvb.com/upload/notMNIST/nmn.png)\n","\n","> some publicly available fonts and extracted glyphs from them to make a dataset similar to MNIST. There are 10 classes, with letters A-J taken from different fonts.\n","\n","> Approaching 0.5% error rate on notMNIST_small would be very impressive. If you run your algorithm on this dataset, please let me know your results.\n","\n","\n","## So, why not MNIST?\n","\n","Many introductions to image classification with deep learning start with MNIST, a standard dataset of handwritten digits. This is unfortunate. Not only does it not produce a “Wow!” effect or show where deep learning shines, but it also can be solved with shallow machine learning techniques. In this case, plain k-Nearest Neighbors produces more than 97% accuracy (or even 99.5% with some data preprocessing!). Moreover, MNIST is not a typical image dataset – and mastering it is unlikely to teach you transferable skills that would be useful for other classification problems\n","\n","> Many good ideas will not work well on MNIST (e.g. batch norm). Inversely many bad ideas may work on MNIST and no[t] transfer to real [computer vision]. - [François Chollet’s tweet](https://twitter.com/fchollet/status/852594987527045120)"]},{"cell_type":"code","metadata":{"id":"jcAAphar__K6"},"source":["!wget http://yaroslavvb.com/upload/notMNIST/notMNIST_small.mat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5bJ8z5dnANsf"},"source":["import matplotlib.pyplot as plt\n","from scipy import io\n","import numpy as np\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rsvxhuP0_mYt"},"source":["## Data Loading"]},{"cell_type":"code","metadata":{"id":"wb9UaA_P_mYu"},"source":["data = io.loadmat(\"notMNIST_small.mat\")\n","\n","# transform data\n","X = data['images']\n","y = data['labels']\n","resolution = 28\n","classes = 10\n","\n","X = np.transpose(X, (2, 0, 1))\n","\n","y = y.astype('int32')\n","X = X.astype('float32') / 255.\n","\n","# shape: (sample, x, y, channel)\n","X = X.reshape((-1, resolution, resolution, 1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IHYFpZ0L_mYw"},"source":["# looking at data; some fonts are strange\n","i = np.random.randint(0, 18724)\n","\n","plt.imshow(X[i,:,:,0])\n","plt.title(\"ABCDEFGHIJ\"[y[i]]);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CeeDSEJv_mYz"},"source":["# random letters\n","rows = 6\n","fig, axs = plt.subplots(rows, classes, figsize=(classes, rows))\n","\n","for letter_id in range(10):\n","    letters = X[y == letter_id]\n","    for i in range(rows):\n","        ax = axs[i, letter_id]\n","        ax.imshow(letters[np.random.randint(len(letters)),:,:,0],\n","                  cmap='Greys', interpolation='none')\n","        ax.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8xmR99PK_mY2"},"source":["# splitting data into training and test sets\n","x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2022)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CzL95VmTBHxx"},"source":["x_train.shape, y_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GnZHnb1uBJWj"},"source":["# Keras를 이용한 모델링 훈련!\n","\n","1. Flatten layer 활용\n","2. Optimizer를 따로 선언해서 사용\n","3. Dropout을 0.2 정도로 활용해볼 것\n","4. 초기값은 default로\n","5. activation이 주어진 Dense layer뒤에 BatchNormalization둬볼 것\n","6. Early stopping도 활용해볼 것"]},{"cell_type":"code","source":[""],"metadata":{"id":"wF_-Qy9x6qva"},"execution_count":null,"outputs":[]}]}